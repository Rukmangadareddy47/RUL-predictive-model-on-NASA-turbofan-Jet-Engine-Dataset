{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed21cf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc3ca1b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unit_nr</th>\n",
       "      <th>time_cycles</th>\n",
       "      <th>setting_1</th>\n",
       "      <th>setting_2</th>\n",
       "      <th>setting_3</th>\n",
       "      <th>s_1</th>\n",
       "      <th>s_2</th>\n",
       "      <th>s_3</th>\n",
       "      <th>s_4</th>\n",
       "      <th>s_5</th>\n",
       "      <th>...</th>\n",
       "      <th>s_12</th>\n",
       "      <th>s_13</th>\n",
       "      <th>s_14</th>\n",
       "      <th>s_15</th>\n",
       "      <th>s_16</th>\n",
       "      <th>s_17</th>\n",
       "      <th>s_18</th>\n",
       "      <th>s_19</th>\n",
       "      <th>s_20</th>\n",
       "      <th>s_21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0007</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.82</td>\n",
       "      <td>1589.70</td>\n",
       "      <td>1400.60</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>521.66</td>\n",
       "      <td>2388.02</td>\n",
       "      <td>8138.62</td>\n",
       "      <td>8.4195</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.06</td>\n",
       "      <td>23.4190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.15</td>\n",
       "      <td>1591.82</td>\n",
       "      <td>1403.14</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.28</td>\n",
       "      <td>2388.07</td>\n",
       "      <td>8131.49</td>\n",
       "      <td>8.4318</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.00</td>\n",
       "      <td>23.4236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.0043</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1587.99</td>\n",
       "      <td>1404.20</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.42</td>\n",
       "      <td>2388.03</td>\n",
       "      <td>8133.23</td>\n",
       "      <td>8.4178</td>\n",
       "      <td>0.03</td>\n",
       "      <td>390</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.95</td>\n",
       "      <td>23.3442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1582.79</td>\n",
       "      <td>1401.87</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.86</td>\n",
       "      <td>2388.08</td>\n",
       "      <td>8133.83</td>\n",
       "      <td>8.3682</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.88</td>\n",
       "      <td>23.3739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.37</td>\n",
       "      <td>1582.85</td>\n",
       "      <td>1406.22</td>\n",
       "      <td>14.62</td>\n",
       "      <td>...</td>\n",
       "      <td>522.19</td>\n",
       "      <td>2388.04</td>\n",
       "      <td>8133.80</td>\n",
       "      <td>8.4294</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.90</td>\n",
       "      <td>23.4044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   unit_nr  time_cycles  setting_1  setting_2  setting_3     s_1     s_2  \\\n",
       "0        1            1    -0.0007    -0.0004      100.0  518.67  641.82   \n",
       "1        1            2     0.0019    -0.0003      100.0  518.67  642.15   \n",
       "2        1            3    -0.0043     0.0003      100.0  518.67  642.35   \n",
       "3        1            4     0.0007     0.0000      100.0  518.67  642.35   \n",
       "4        1            5    -0.0019    -0.0002      100.0  518.67  642.37   \n",
       "\n",
       "       s_3      s_4    s_5  ...    s_12     s_13     s_14    s_15  s_16  s_17  \\\n",
       "0  1589.70  1400.60  14.62  ...  521.66  2388.02  8138.62  8.4195  0.03   392   \n",
       "1  1591.82  1403.14  14.62  ...  522.28  2388.07  8131.49  8.4318  0.03   392   \n",
       "2  1587.99  1404.20  14.62  ...  522.42  2388.03  8133.23  8.4178  0.03   390   \n",
       "3  1582.79  1401.87  14.62  ...  522.86  2388.08  8133.83  8.3682  0.03   392   \n",
       "4  1582.85  1406.22  14.62  ...  522.19  2388.04  8133.80  8.4294  0.03   393   \n",
       "\n",
       "   s_18   s_19   s_20     s_21  \n",
       "0  2388  100.0  39.06  23.4190  \n",
       "1  2388  100.0  39.00  23.4236  \n",
       "2  2388  100.0  38.95  23.3442  \n",
       "3  2388  100.0  38.88  23.3739  \n",
       "4  2388  100.0  38.90  23.4044  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define column names for easy indexing\n",
    "index_names = ['unit_nr', 'time_cycles']\n",
    "setting_names = ['setting_1', 'setting_2', 'setting_3']\n",
    "sensor_names = ['s_{}'.format(i) for i in range(1,22)] \n",
    "col_names = index_names + setting_names + sensor_names\n",
    "\n",
    "# read data\n",
    "train = pd.read_csv('train_FD001.txt', sep='\\s+', header=None, names=col_names)\n",
    "test = pd.read_csv('test_FD001.txt', sep='\\s+', header=None, names=col_names)\n",
    "y_test = pd.read_csv('RUL_FD001.txt', sep='\\s+', header=None, names=['RUL'])\n",
    "\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abedf0db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unit_nr</th>\n",
       "      <th>time_cycles</th>\n",
       "      <th>RUL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   unit_nr  time_cycles  RUL\n",
       "0        1            1  191\n",
       "1        1            2  190\n",
       "2        1            3  189\n",
       "3        1            4  188\n",
       "4        1            5  187"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_remaining_useful_life(df):\n",
    "    # Get the total number of cycles for each unit\n",
    "    grouped_by_unit = df.groupby(by=\"unit_nr\")\n",
    "    max_cycle = grouped_by_unit[\"time_cycles\"].max()\n",
    "    \n",
    "    # Merge the max cycle back into the original frame\n",
    "    result_frame = df.merge(max_cycle.to_frame(name='max_cycle'), left_on='unit_nr', right_index=True)\n",
    "    \n",
    "    # Calculate remaining useful life for each row\n",
    "    remaining_useful_life = result_frame[\"max_cycle\"] - result_frame[\"time_cycles\"]\n",
    "    result_frame[\"RUL\"] = remaining_useful_life\n",
    "    \n",
    "    # drop max_cycle as it's no longer needed\n",
    "    result_frame = result_frame.drop(\"max_cycle\", axis=1)\n",
    "    return result_frame\n",
    "\n",
    "train = add_remaining_useful_life(train)\n",
    "train[index_names+['RUL']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "deed357d",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_labels = index_names+setting_names\n",
    "#dropping the columns except the sensor datas\n",
    "X_train = train.drop(drop_labels, axis=1)\n",
    "y_train = X_train.pop('RUL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "197c7d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test.groupby('unit_nr').last().reset_index().drop(drop_labels, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "592a35e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s_1</th>\n",
       "      <th>s_2</th>\n",
       "      <th>s_3</th>\n",
       "      <th>s_4</th>\n",
       "      <th>s_5</th>\n",
       "      <th>s_6</th>\n",
       "      <th>s_7</th>\n",
       "      <th>s_8</th>\n",
       "      <th>s_9</th>\n",
       "      <th>s_10</th>\n",
       "      <th>...</th>\n",
       "      <th>s_12</th>\n",
       "      <th>s_13</th>\n",
       "      <th>s_14</th>\n",
       "      <th>s_15</th>\n",
       "      <th>s_16</th>\n",
       "      <th>s_17</th>\n",
       "      <th>s_18</th>\n",
       "      <th>s_19</th>\n",
       "      <th>s_20</th>\n",
       "      <th>s_21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>518.67</td>\n",
       "      <td>642.58</td>\n",
       "      <td>1581.22</td>\n",
       "      <td>1398.91</td>\n",
       "      <td>14.62</td>\n",
       "      <td>21.61</td>\n",
       "      <td>554.42</td>\n",
       "      <td>2388.08</td>\n",
       "      <td>9056.40</td>\n",
       "      <td>1.3</td>\n",
       "      <td>...</td>\n",
       "      <td>521.79</td>\n",
       "      <td>2388.06</td>\n",
       "      <td>8130.11</td>\n",
       "      <td>8.4024</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.81</td>\n",
       "      <td>23.3552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>518.67</td>\n",
       "      <td>642.55</td>\n",
       "      <td>1586.59</td>\n",
       "      <td>1410.83</td>\n",
       "      <td>14.62</td>\n",
       "      <td>21.61</td>\n",
       "      <td>553.52</td>\n",
       "      <td>2388.10</td>\n",
       "      <td>9044.77</td>\n",
       "      <td>1.3</td>\n",
       "      <td>...</td>\n",
       "      <td>521.74</td>\n",
       "      <td>2388.09</td>\n",
       "      <td>8126.90</td>\n",
       "      <td>8.4505</td>\n",
       "      <td>0.03</td>\n",
       "      <td>391</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.81</td>\n",
       "      <td>23.2618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>518.67</td>\n",
       "      <td>642.88</td>\n",
       "      <td>1589.75</td>\n",
       "      <td>1418.89</td>\n",
       "      <td>14.62</td>\n",
       "      <td>21.61</td>\n",
       "      <td>552.59</td>\n",
       "      <td>2388.16</td>\n",
       "      <td>9049.26</td>\n",
       "      <td>1.3</td>\n",
       "      <td>...</td>\n",
       "      <td>520.83</td>\n",
       "      <td>2388.14</td>\n",
       "      <td>8131.46</td>\n",
       "      <td>8.4119</td>\n",
       "      <td>0.03</td>\n",
       "      <td>395</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.93</td>\n",
       "      <td>23.2740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>518.67</td>\n",
       "      <td>642.78</td>\n",
       "      <td>1594.53</td>\n",
       "      <td>1406.88</td>\n",
       "      <td>14.62</td>\n",
       "      <td>21.61</td>\n",
       "      <td>552.64</td>\n",
       "      <td>2388.13</td>\n",
       "      <td>9051.30</td>\n",
       "      <td>1.3</td>\n",
       "      <td>...</td>\n",
       "      <td>521.88</td>\n",
       "      <td>2388.11</td>\n",
       "      <td>8133.64</td>\n",
       "      <td>8.4634</td>\n",
       "      <td>0.03</td>\n",
       "      <td>395</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.58</td>\n",
       "      <td>23.2581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>518.67</td>\n",
       "      <td>642.27</td>\n",
       "      <td>1589.94</td>\n",
       "      <td>1419.36</td>\n",
       "      <td>14.62</td>\n",
       "      <td>21.61</td>\n",
       "      <td>553.29</td>\n",
       "      <td>2388.10</td>\n",
       "      <td>9053.99</td>\n",
       "      <td>1.3</td>\n",
       "      <td>...</td>\n",
       "      <td>521.00</td>\n",
       "      <td>2388.15</td>\n",
       "      <td>8125.74</td>\n",
       "      <td>8.4362</td>\n",
       "      <td>0.03</td>\n",
       "      <td>394</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.75</td>\n",
       "      <td>23.4117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>518.67</td>\n",
       "      <td>642.30</td>\n",
       "      <td>1590.88</td>\n",
       "      <td>1397.94</td>\n",
       "      <td>14.62</td>\n",
       "      <td>21.61</td>\n",
       "      <td>553.99</td>\n",
       "      <td>2388.03</td>\n",
       "      <td>9062.41</td>\n",
       "      <td>1.3</td>\n",
       "      <td>...</td>\n",
       "      <td>522.30</td>\n",
       "      <td>2388.01</td>\n",
       "      <td>8148.24</td>\n",
       "      <td>8.4110</td>\n",
       "      <td>0.03</td>\n",
       "      <td>391</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.96</td>\n",
       "      <td>23.4606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>518.67</td>\n",
       "      <td>642.59</td>\n",
       "      <td>1582.96</td>\n",
       "      <td>1410.92</td>\n",
       "      <td>14.62</td>\n",
       "      <td>21.61</td>\n",
       "      <td>554.05</td>\n",
       "      <td>2388.06</td>\n",
       "      <td>9076.36</td>\n",
       "      <td>1.3</td>\n",
       "      <td>...</td>\n",
       "      <td>521.58</td>\n",
       "      <td>2388.06</td>\n",
       "      <td>8155.48</td>\n",
       "      <td>8.4500</td>\n",
       "      <td>0.03</td>\n",
       "      <td>395</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.61</td>\n",
       "      <td>23.2953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>518.67</td>\n",
       "      <td>642.68</td>\n",
       "      <td>1599.51</td>\n",
       "      <td>1415.47</td>\n",
       "      <td>14.62</td>\n",
       "      <td>21.61</td>\n",
       "      <td>553.44</td>\n",
       "      <td>2388.13</td>\n",
       "      <td>9062.34</td>\n",
       "      <td>1.3</td>\n",
       "      <td>...</td>\n",
       "      <td>521.53</td>\n",
       "      <td>2388.09</td>\n",
       "      <td>8146.39</td>\n",
       "      <td>8.4235</td>\n",
       "      <td>0.03</td>\n",
       "      <td>394</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.76</td>\n",
       "      <td>23.3608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>518.67</td>\n",
       "      <td>642.00</td>\n",
       "      <td>1585.03</td>\n",
       "      <td>1397.98</td>\n",
       "      <td>14.62</td>\n",
       "      <td>21.61</td>\n",
       "      <td>554.75</td>\n",
       "      <td>2388.01</td>\n",
       "      <td>9067.16</td>\n",
       "      <td>1.3</td>\n",
       "      <td>...</td>\n",
       "      <td>521.82</td>\n",
       "      <td>2388.02</td>\n",
       "      <td>8150.38</td>\n",
       "      <td>8.4003</td>\n",
       "      <td>0.03</td>\n",
       "      <td>391</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.95</td>\n",
       "      <td>23.3595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>518.67</td>\n",
       "      <td>642.95</td>\n",
       "      <td>1601.62</td>\n",
       "      <td>1424.99</td>\n",
       "      <td>14.62</td>\n",
       "      <td>21.61</td>\n",
       "      <td>552.48</td>\n",
       "      <td>2388.06</td>\n",
       "      <td>9155.03</td>\n",
       "      <td>1.3</td>\n",
       "      <td>...</td>\n",
       "      <td>521.07</td>\n",
       "      <td>2388.05</td>\n",
       "      <td>8214.64</td>\n",
       "      <td>8.4903</td>\n",
       "      <td>0.03</td>\n",
       "      <td>396</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.70</td>\n",
       "      <td>23.1855</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       s_1     s_2      s_3      s_4    s_5    s_6     s_7      s_8      s_9  \\\n",
       "0   518.67  642.58  1581.22  1398.91  14.62  21.61  554.42  2388.08  9056.40   \n",
       "1   518.67  642.55  1586.59  1410.83  14.62  21.61  553.52  2388.10  9044.77   \n",
       "2   518.67  642.88  1589.75  1418.89  14.62  21.61  552.59  2388.16  9049.26   \n",
       "3   518.67  642.78  1594.53  1406.88  14.62  21.61  552.64  2388.13  9051.30   \n",
       "4   518.67  642.27  1589.94  1419.36  14.62  21.61  553.29  2388.10  9053.99   \n",
       "..     ...     ...      ...      ...    ...    ...     ...      ...      ...   \n",
       "95  518.67  642.30  1590.88  1397.94  14.62  21.61  553.99  2388.03  9062.41   \n",
       "96  518.67  642.59  1582.96  1410.92  14.62  21.61  554.05  2388.06  9076.36   \n",
       "97  518.67  642.68  1599.51  1415.47  14.62  21.61  553.44  2388.13  9062.34   \n",
       "98  518.67  642.00  1585.03  1397.98  14.62  21.61  554.75  2388.01  9067.16   \n",
       "99  518.67  642.95  1601.62  1424.99  14.62  21.61  552.48  2388.06  9155.03   \n",
       "\n",
       "    s_10  ...    s_12     s_13     s_14    s_15  s_16  s_17  s_18   s_19  \\\n",
       "0    1.3  ...  521.79  2388.06  8130.11  8.4024  0.03   393  2388  100.0   \n",
       "1    1.3  ...  521.74  2388.09  8126.90  8.4505  0.03   391  2388  100.0   \n",
       "2    1.3  ...  520.83  2388.14  8131.46  8.4119  0.03   395  2388  100.0   \n",
       "3    1.3  ...  521.88  2388.11  8133.64  8.4634  0.03   395  2388  100.0   \n",
       "4    1.3  ...  521.00  2388.15  8125.74  8.4362  0.03   394  2388  100.0   \n",
       "..   ...  ...     ...      ...      ...     ...   ...   ...   ...    ...   \n",
       "95   1.3  ...  522.30  2388.01  8148.24  8.4110  0.03   391  2388  100.0   \n",
       "96   1.3  ...  521.58  2388.06  8155.48  8.4500  0.03   395  2388  100.0   \n",
       "97   1.3  ...  521.53  2388.09  8146.39  8.4235  0.03   394  2388  100.0   \n",
       "98   1.3  ...  521.82  2388.02  8150.38  8.4003  0.03   391  2388  100.0   \n",
       "99   1.3  ...  521.07  2388.05  8214.64  8.4903  0.03   396  2388  100.0   \n",
       "\n",
       "     s_20     s_21  \n",
       "0   38.81  23.3552  \n",
       "1   38.81  23.2618  \n",
       "2   38.93  23.2740  \n",
       "3   38.58  23.2581  \n",
       "4   38.75  23.4117  \n",
       "..    ...      ...  \n",
       "95  38.96  23.4606  \n",
       "96  38.61  23.2953  \n",
       "97  38.76  23.3608  \n",
       "98  38.95  23.3595  \n",
       "99  38.70  23.1855  \n",
       "\n",
       "[100 rows x 21 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19a3a970",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau,EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b71218e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_trains, X_val, y_trains, y_val = train_test_split(X_train, y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "523919f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(Dense(21, activation='relu', input_shape=(21,)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e8336be",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.1\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=100000,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b1ef3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_A=tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
    "model.compile(optimizer=optimizer_A,loss='mean_absolute_error',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff0fdd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 21)                462       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 21)               84        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                704       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1024)              33792     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                32832     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 592,739\n",
      "Trainable params: 592,697\n",
      "Non-trainable params: 42\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5f078fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
    "                              patience=3, min_lr=1e-7, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4f40ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "645/645 [==============================] - 10s 9ms/step - loss: 39.1803 - accuracy: 0.0048 - val_loss: 82.7010 - val_accuracy: 0.0046 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "645/645 [==============================] - 6s 10ms/step - loss: 33.7347 - accuracy: 0.0048 - val_loss: 101.6772 - val_accuracy: 0.0046 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "645/645 [==============================] - 6s 9ms/step - loss: 32.7917 - accuracy: 0.0048 - val_loss: 93.5753 - val_accuracy: 0.0046 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "642/645 [============================>.] - ETA: 0s - loss: 32.3198 - accuracy: 0.0049\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "645/645 [==============================] - 6s 9ms/step - loss: 32.3272 - accuracy: 0.0048 - val_loss: 91.6568 - val_accuracy: 0.0046 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "645/645 [==============================] - 6s 9ms/step - loss: 31.7998 - accuracy: 0.0048 - val_loss: 234.2764 - val_accuracy: 0.0046 - lr: 1.0000e-04\n",
      "Epoch 6/50\n",
      "645/645 [==============================] - 6s 9ms/step - loss: 31.7548 - accuracy: 0.0048 - val_loss: 104.1723 - val_accuracy: 0.0046 - lr: 1.0000e-04\n",
      "Epoch 7/50\n",
      "641/645 [============================>.] - ETA: 0s - loss: 31.6824 - accuracy: 0.0049\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "645/645 [==============================] - 6s 9ms/step - loss: 31.6573 - accuracy: 0.0048 - val_loss: 87.7727 - val_accuracy: 0.0046 - lr: 1.0000e-04\n",
      "Epoch 8/50\n",
      "645/645 [==============================] - 6s 10ms/step - loss: 31.6978 - accuracy: 0.0048 - val_loss: 32.0828 - val_accuracy: 0.0046 - lr: 1.0000e-05\n",
      "Epoch 9/50\n",
      "645/645 [==============================] - 7s 10ms/step - loss: 31.5482 - accuracy: 0.0048 - val_loss: 31.4634 - val_accuracy: 0.0046 - lr: 1.0000e-05\n",
      "Epoch 10/50\n",
      "645/645 [==============================] - 6s 10ms/step - loss: 31.3172 - accuracy: 0.0048 - val_loss: 34.4001 - val_accuracy: 0.0046 - lr: 1.0000e-05\n",
      "Epoch 11/50\n",
      "645/645 [==============================] - 6s 10ms/step - loss: 31.4099 - accuracy: 0.0048 - val_loss: 30.5322 - val_accuracy: 0.0046 - lr: 1.0000e-05\n",
      "Epoch 12/50\n",
      "645/645 [==============================] - 6s 10ms/step - loss: 31.4018 - accuracy: 0.0048 - val_loss: 36.0995 - val_accuracy: 0.0046 - lr: 1.0000e-05\n",
      "Epoch 13/50\n",
      "645/645 [==============================] - 6s 9ms/step - loss: 31.5362 - accuracy: 0.0048 - val_loss: 39.1468 - val_accuracy: 0.0046 - lr: 1.0000e-05\n",
      "Epoch 14/50\n",
      "643/645 [============================>.] - ETA: 0s - loss: 31.4770 - accuracy: 0.0049\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "645/645 [==============================] - 7s 10ms/step - loss: 31.4810 - accuracy: 0.0048 - val_loss: 48.4605 - val_accuracy: 0.0046 - lr: 1.0000e-05\n",
      "Epoch 15/50\n",
      "645/645 [==============================] - 6s 9ms/step - loss: 31.4366 - accuracy: 0.0048 - val_loss: 30.0303 - val_accuracy: 0.0046 - lr: 1.0000e-06\n",
      "Epoch 16/50\n",
      "645/645 [==============================] - 6s 9ms/step - loss: 31.3800 - accuracy: 0.0048 - val_loss: 30.3908 - val_accuracy: 0.0046 - lr: 1.0000e-06\n",
      "Epoch 17/50\n",
      "645/645 [==============================] - 6s 9ms/step - loss: 31.5472 - accuracy: 0.0048 - val_loss: 30.0466 - val_accuracy: 0.0046 - lr: 1.0000e-06\n",
      "Epoch 18/50\n",
      "645/645 [==============================] - 8s 12ms/step - loss: 31.7224 - accuracy: 0.0048 - val_loss: 29.9338 - val_accuracy: 0.0046 - lr: 1.0000e-06\n",
      "Epoch 19/50\n",
      "645/645 [==============================] - 7s 10ms/step - loss: 31.3719 - accuracy: 0.0048 - val_loss: 30.2293 - val_accuracy: 0.0046 - lr: 1.0000e-06\n",
      "Epoch 20/50\n",
      "645/645 [==============================] - 7s 12ms/step - loss: 31.3936 - accuracy: 0.0048 - val_loss: 29.9064 - val_accuracy: 0.0046 - lr: 1.0000e-06\n",
      "Epoch 21/50\n",
      "645/645 [==============================] - 8s 12ms/step - loss: 31.3290 - accuracy: 0.0048 - val_loss: 29.9399 - val_accuracy: 0.0046 - lr: 1.0000e-06\n",
      "Epoch 22/50\n",
      "645/645 [==============================] - 6s 10ms/step - loss: 31.4292 - accuracy: 0.0048 - val_loss: 29.9653 - val_accuracy: 0.0046 - lr: 1.0000e-06\n",
      "Epoch 23/50\n",
      "643/645 [============================>.] - ETA: 0s - loss: 31.3750 - accuracy: 0.0048\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "645/645 [==============================] - 7s 10ms/step - loss: 31.3833 - accuracy: 0.0048 - val_loss: 29.9342 - val_accuracy: 0.0046 - lr: 1.0000e-06\n",
      "Epoch 24/50\n",
      "645/645 [==============================] - 6s 10ms/step - loss: 31.1689 - accuracy: 0.0048 - val_loss: 29.9331 - val_accuracy: 0.0046 - lr: 1.0000e-07\n",
      "Epoch 25/50\n",
      "645/645 [==============================] - 6s 9ms/step - loss: 31.4562 - accuracy: 0.0048 - val_loss: 29.9588 - val_accuracy: 0.0046 - lr: 1.0000e-07\n",
      "Epoch 26/50\n",
      "644/645 [============================>.] - ETA: 0s - loss: 31.8075 - accuracy: 0.0048\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 31.8053 - accuracy: 0.0048 - val_loss: 29.9375 - val_accuracy: 0.0046 - lr: 1.0000e-07\n",
      "Epoch 27/50\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 31.5009 - accuracy: 0.0048 - val_loss: 29.9860 - val_accuracy: 0.0046 - lr: 1.0000e-07\n",
      "Epoch 28/50\n",
      "645/645 [==============================] - 7s 11ms/step - loss: 31.3065 - accuracy: 0.0048 - val_loss: 29.9083 - val_accuracy: 0.0046 - lr: 1.0000e-07\n",
      "Epoch 29/50\n",
      "645/645 [==============================] - 6s 9ms/step - loss: 31.3557 - accuracy: 0.0048 - val_loss: 29.9079 - val_accuracy: 0.0046 - lr: 1.0000e-07\n",
      "Epoch 30/50\n",
      "645/645 [==============================] - 6s 9ms/step - loss: 31.4672 - accuracy: 0.0048 - val_loss: 29.9313 - val_accuracy: 0.0046 - lr: 1.0000e-07\n",
      "Epoch 31/50\n",
      "645/645 [==============================] - 6s 9ms/step - loss: 31.2166 - accuracy: 0.0048 - val_loss: 29.9100 - val_accuracy: 0.0046 - lr: 1.0000e-07\n",
      "Epoch 32/50\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 31.3930 - accuracy: 0.0048 - val_loss: 29.9405 - val_accuracy: 0.0046 - lr: 1.0000e-07\n",
      "Epoch 33/50\n",
      "645/645 [==============================] - 6s 9ms/step - loss: 31.3590 - accuracy: 0.0048 - val_loss: 29.9599 - val_accuracy: 0.0046 - lr: 1.0000e-07\n",
      "Epoch 34/50\n",
      "645/645 [==============================] - 6s 10ms/step - loss: 31.5460 - accuracy: 0.0048 - val_loss: 29.9263 - val_accuracy: 0.0046 - lr: 1.0000e-07\n",
      "Epoch 35/50\n",
      "645/645 [==============================] - 6s 9ms/step - loss: 31.4091 - accuracy: 0.0048 - val_loss: 29.9159 - val_accuracy: 0.0046 - lr: 1.0000e-07\n",
      "Epoch 36/50\n",
      "645/645 [==============================] - 6s 10ms/step - loss: 31.5076 - accuracy: 0.0048 - val_loss: 29.9447 - val_accuracy: 0.0046 - lr: 1.0000e-07\n",
      "Epoch 37/50\n",
      "645/645 [==============================] - 6s 9ms/step - loss: 31.4566 - accuracy: 0.0048 - val_loss: 29.9548 - val_accuracy: 0.0046 - lr: 1.0000e-07\n",
      "Epoch 38/50\n",
      "645/645 [==============================] - 6s 9ms/step - loss: 31.5315 - accuracy: 0.0048 - val_loss: 29.9756 - val_accuracy: 0.0046 - lr: 1.0000e-07\n",
      "Epoch 39/50\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 31.3046 - accuracy: 0.0048 - val_loss: 29.9512 - val_accuracy: 0.0046 - lr: 1.0000e-07\n",
      "Epoch 40/50\n",
      "645/645 [==============================] - 6s 9ms/step - loss: 31.4161 - accuracy: 0.0048 - val_loss: 29.9525 - val_accuracy: 0.0046 - lr: 1.0000e-07\n",
      "Epoch 41/50\n",
      "645/645 [==============================] - 6s 9ms/step - loss: 31.3047 - accuracy: 0.0048 - val_loss: 29.9589 - val_accuracy: 0.0046 - lr: 1.0000e-07\n",
      "Epoch 42/50\n",
      "645/645 [==============================] - 6s 9ms/step - loss: 31.5282 - accuracy: 0.0048 - val_loss: 29.8982 - val_accuracy: 0.0046 - lr: 1.0000e-07\n",
      "Epoch 43/50\n",
      "645/645 [==============================] - 6s 9ms/step - loss: 31.5396 - accuracy: 0.0048 - val_loss: 29.9367 - val_accuracy: 0.0046 - lr: 1.0000e-07\n",
      "Epoch 44/50\n",
      "645/645 [==============================] - 6s 9ms/step - loss: 31.5464 - accuracy: 0.0048 - val_loss: 29.9286 - val_accuracy: 0.0046 - lr: 1.0000e-07\n",
      "Epoch 45/50\n",
      "645/645 [==============================] - 6s 9ms/step - loss: 31.4655 - accuracy: 0.0048 - val_loss: 29.9379 - val_accuracy: 0.0046 - lr: 1.0000e-07\n",
      "Epoch 46/50\n",
      "645/645 [==============================] - 6s 9ms/step - loss: 31.3763 - accuracy: 0.0048 - val_loss: 29.9553 - val_accuracy: 0.0046 - lr: 1.0000e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50\n",
      "645/645 [==============================] - 6s 9ms/step - loss: 31.4864 - accuracy: 0.0048 - val_loss: 29.9165 - val_accuracy: 0.0046 - lr: 1.0000e-07\n",
      "Epoch 48/50\n",
      "645/645 [==============================] - 6s 9ms/step - loss: 31.5995 - accuracy: 0.0048 - val_loss: 29.9153 - val_accuracy: 0.0046 - lr: 1.0000e-07\n",
      "Epoch 49/50\n",
      "645/645 [==============================] - 6s 10ms/step - loss: 31.6504 - accuracy: 0.0048 - val_loss: 29.9620 - val_accuracy: 0.0046 - lr: 1.0000e-07\n",
      "Epoch 50/50\n",
      "645/645 [==============================] - 6s 9ms/step - loss: 31.4320 - accuracy: 0.0048 - val_loss: 29.9373 - val_accuracy: 0.0046 - lr: 1.0000e-07\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=X_train,y=y_train,\n",
    "                    validation_data = (X_val,y_val),\n",
    "                    epochs = 50,\n",
    "                    shuffle = True,\n",
    "                    callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8cca1b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "loss_train = history.history['loss']\n",
    "loss_val = history.history['val_loss']\n",
    "epochs = range(1,51)\n",
    "plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
    "plt.plot(epochs, loss_val, 'b', label='validation loss')\n",
    "plt.title('Training and Validation loss for Adam')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aceb051e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 3s 3ms/step\n",
      "[[153.84276  ]\n",
      " [139.19429  ]\n",
      " [ 57.211517 ]\n",
      " [111.5113   ]\n",
      " [111.24104  ]\n",
      " [126.13121  ]\n",
      " [121.16729  ]\n",
      " [ 95.274826 ]\n",
      " [119.901    ]\n",
      " [119.721664 ]\n",
      " [ 90.332016 ]\n",
      " [ 87.19435  ]\n",
      " [ 93.28754  ]\n",
      " [123.039955 ]\n",
      " [148.38484  ]\n",
      " [134.4788   ]\n",
      " [ 52.36517  ]\n",
      " [ 64.70228  ]\n",
      " [127.92745  ]\n",
      " [ 19.657246 ]\n",
      " [ 65.53322  ]\n",
      " [142.08308  ]\n",
      " [151.30177  ]\n",
      " [ 28.757582 ]\n",
      " [141.13475  ]\n",
      " [126.43053  ]\n",
      " [141.22414  ]\n",
      " [109.19618  ]\n",
      " [100.627396 ]\n",
      " [107.35507  ]\n",
      " [ 15.7656555]\n",
      " [ 60.206516 ]\n",
      " [112.482315 ]\n",
      " [  8.803218 ]\n",
      " [ 11.761944 ]\n",
      " [ 25.642853 ]\n",
      " [ 49.06641  ]\n",
      " [ 53.172684 ]\n",
      " [139.15842  ]\n",
      " [ 28.370113 ]\n",
      " [ 70.38962  ]\n",
      " [ 16.80953  ]\n",
      " [ 56.543533 ]\n",
      " [128.12032  ]\n",
      " [ 65.71008  ]\n",
      " [ 53.438545 ]\n",
      " [142.73203  ]\n",
      " [128.97859  ]\n",
      " [ 14.843633 ]\n",
      " [127.60085  ]\n",
      " [131.29614  ]\n",
      " [ 36.493256 ]\n",
      " [ 39.82647  ]\n",
      " [151.3214   ]\n",
      " [138.5529   ]\n",
      " [ 15.118859 ]\n",
      " [125.04013  ]\n",
      " [ 52.93606  ]\n",
      " [152.19118  ]\n",
      " [125.24897  ]\n",
      " [ 44.200783 ]\n",
      " [ 58.788403 ]\n",
      " [ 69.45023  ]\n",
      " [ 42.641045 ]\n",
      " [152.70802  ]\n",
      " [ 22.78451  ]\n",
      " [146.91154  ]\n",
      " [ 20.448582 ]\n",
      " [142.2372   ]\n",
      " [118.36258  ]\n",
      " [132.58427  ]\n",
      " [ 75.097855 ]\n",
      " [155.96204  ]\n",
      " [ 99.976105 ]\n",
      " [145.68845  ]\n",
      " [ 15.491203 ]\n",
      " [ 31.185331 ]\n",
      " [165.74873  ]\n",
      " [137.34196  ]\n",
      " [ 75.43982  ]\n",
      " [  9.6784725]\n",
      " [  8.641738 ]\n",
      " [130.37247  ]\n",
      " [ 56.228977 ]\n",
      " [132.2726   ]\n",
      " [ 84.312256 ]\n",
      " [148.58665  ]\n",
      " [137.70113  ]\n",
      " [121.913666 ]\n",
      " [ 42.085274 ]\n",
      " [ 28.669563 ]\n",
      " [ 14.292824 ]\n",
      " [ 50.545013 ]\n",
      " [ 51.64533  ]\n",
      " [143.00917  ]\n",
      " [147.3554   ]\n",
      " [ 78.04208  ]\n",
      " [ 86.75517  ]\n",
      " [143.54977  ]\n",
      " [ 22.09103  ]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d1af0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5954032128930392\n",
      "19.436571102142334\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "\n",
    "print(sklearn.metrics.r2_score(y_test, y_pred))\n",
    "#print(sklearn.metrics.mean_absolute_percentage_error(y_test, y_pred))\n",
    "print(sklearn.metrics.mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b155d685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting autokeras\n",
      "  Downloading https://files.pythonhosted.org/packages/40/cd/aa8102a3b6d5db82aea5870a7d18457456953990d6286f61731723b7de9a/autokeras-1.0.19-py3-none-any.whl (162kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from autokeras) (1.3.5)\n",
      "Requirement already satisfied: packaging in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from autokeras) (21.3)\n",
      "Collecting keras-tuner>=1.1.0 (from autokeras)\n",
      "  Downloading https://files.pythonhosted.org/packages/7f/6c/8ac746c9d0e7776bf119c511c98ce10f04e9b9d7c37cb8c0ea226b0968d1/keras_tuner-1.1.3-py3-none-any.whl (135kB)\n",
      "Requirement already satisfied: tensorflow>=2.8.0 in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from autokeras) (2.9.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas->autokeras) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.17.3; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version < \"3.10\" in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas->autokeras) (1.21.4)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas->autokeras) (2021.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from packaging->autokeras) (3.0.6)\n",
      "Requirement already satisfied: tensorboard in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from keras-tuner>=1.1.0->autokeras) (2.9.1)\n",
      "Collecting kt-legacy (from keras-tuner>=1.1.0->autokeras)\n",
      "  Downloading https://files.pythonhosted.org/packages/09/83/7c3001c7826cd7194f36dc971c57a25c7a55373e7087c62a6d1d1193c022/kt_legacy-1.0.4-py3-none-any.whl\n",
      "Requirement already satisfied: requests in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from keras-tuner>=1.1.0->autokeras) (2.28.0)\n",
      "Requirement already satisfied: ipython in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from keras-tuner>=1.1.0->autokeras) (7.30.1)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow>=2.8.0->autokeras) (0.4.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow>=2.8.0->autokeras) (1.14.1)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow>=2.8.0->autokeras) (1.12)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow>=2.8.0->autokeras) (2.9.0)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow>=2.8.0->autokeras) (2.9.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow>=2.8.0->autokeras) (1.46.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow>=2.8.0->autokeras) (4.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow>=2.8.0->autokeras) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow>=2.8.0->autokeras) (0.26.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow>=2.8.0->autokeras) (3.7.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow>=2.8.0->autokeras) (1.1.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow>=2.8.0->autokeras) (1.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow>=2.8.0->autokeras) (1.6.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow>=2.8.0->autokeras) (0.2.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow>=2.8.0->autokeras) (1.1.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow>=2.8.0->autokeras) (14.0.1)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow>=2.8.0->autokeras) (3.19.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow>=2.8.0->autokeras) (3.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorflow>=2.8.0->autokeras) (41.2.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorboard->keras-tuner>=1.1.0->autokeras) (2.1.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorboard->keras-tuner>=1.1.0->autokeras) (3.3.7)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorboard->keras-tuner>=1.1.0->autokeras) (0.37.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorboard->keras-tuner>=1.1.0->autokeras) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorboard->keras-tuner>=1.1.0->autokeras) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorboard->keras-tuner>=1.1.0->autokeras) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tensorboard->keras-tuner>=1.1.0->autokeras) (2.8.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->keras-tuner>=1.1.0->autokeras) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->keras-tuner>=1.1.0->autokeras) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->keras-tuner>=1.1.0->autokeras) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->keras-tuner>=1.1.0->autokeras) (3.3)\n",
      "Requirement already satisfied: traitlets>=4.2 in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from ipython->keras-tuner>=1.1.0->autokeras) (5.1.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from ipython->keras-tuner>=1.1.0->autokeras) (0.1.3)\n",
      "Requirement already satisfied: pygments in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from ipython->keras-tuner>=1.1.0->autokeras) (2.10.0)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from ipython->keras-tuner>=1.1.0->autokeras) (0.7.5)\n",
      "Requirement already satisfied: decorator in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from ipython->keras-tuner>=1.1.0->autokeras) (5.1.0)\n",
      "Requirement already satisfied: backcall in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from ipython->keras-tuner>=1.1.0->autokeras) (0.2.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from ipython->keras-tuner>=1.1.0->autokeras) (3.0.24)\n",
      "Requirement already satisfied: colorama; sys_platform == \"win32\" in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from ipython->keras-tuner>=1.1.0->autokeras) (0.4.4)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from ipython->keras-tuner>=1.1.0->autokeras) (0.18.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from markdown>=2.6.8->tensorboard->keras-tuner>=1.1.0->autokeras) (4.11.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner>=1.1.0->autokeras) (1.3.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner>=1.1.0->autokeras) (5.2.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner>=1.1.0->autokeras) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner>=1.1.0->autokeras) (0.2.8)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->keras-tuner>=1.1.0->autokeras) (0.2.5)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from jedi>=0.16->ipython->keras-tuner>=1.1.0->autokeras) (0.8.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.2.3, however version 22.2.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard->keras-tuner>=1.1.0->autokeras) (3.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner>=1.1.0->autokeras) (3.2.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\rukku\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<3,>=1.6.3->tensorboard->keras-tuner>=1.1.0->autokeras) (0.4.8)\n",
      "Installing collected packages: kt-legacy, keras-tuner, autokeras\n",
      "Successfully installed autokeras-1.0.19 keras-tuner-1.1.3 kt-legacy-1.0.4\n"
     ]
    }
   ],
   "source": [
    "!pip install autokeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53a1ef29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autokeras import StructuredDataRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5cb7c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 15 Complete [00h 09m 12s]\n",
      "val_loss: 38.27334213256836\n",
      "\n",
      "Best val_loss So Far: 38.27334213256836\n",
      "Total elapsed time: 00h 50m 14s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Epoch 1/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 107.6374 - mean_squared_error: 16327.9287\n",
      "Epoch 2/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 107.2734 - mean_squared_error: 16254.2490\n",
      "Epoch 3/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 106.8347 - mean_squared_error: 16162.4277\n",
      "Epoch 4/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 106.2605 - mean_squared_error: 16039.3555\n",
      "Epoch 5/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 105.4508 - mean_squared_error: 15861.1426\n",
      "Epoch 6/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 104.2246 - mean_squared_error: 15581.3193\n",
      "Epoch 7/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 102.2318 - mean_squared_error: 15104.2588\n",
      "Epoch 8/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 98.5972 - mean_squared_error: 14200.0059\n",
      "Epoch 9/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 90.5036 - mean_squared_error: 12175.3057\n",
      "Epoch 10/152\n",
      "645/645 [==============================] - 4s 7ms/step - loss: 64.9990 - mean_squared_error: 6828.6436\n",
      "Epoch 11/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 39.9125 - mean_squared_error: 2824.0288\n",
      "Epoch 12/152\n",
      "645/645 [==============================] - 4s 5ms/step - loss: 36.9732 - mean_squared_error: 2460.4341\n",
      "Epoch 13/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 35.6515 - mean_squared_error: 2325.8936\n",
      "Epoch 14/152\n",
      "645/645 [==============================] - 3s 5ms/step - loss: 34.8676 - mean_squared_error: 2251.4375\n",
      "Epoch 15/152\n",
      "645/645 [==============================] - 4s 5ms/step - loss: 34.2958 - mean_squared_error: 2199.1570\n",
      "Epoch 16/152\n",
      "645/645 [==============================] - 4s 5ms/step - loss: 33.8567 - mean_squared_error: 2162.8892\n",
      "Epoch 17/152\n",
      "645/645 [==============================] - 3s 5ms/step - loss: 33.4893 - mean_squared_error: 2132.3691\n",
      "Epoch 18/152\n",
      "645/645 [==============================] - 3s 5ms/step - loss: 33.1758 - mean_squared_error: 2106.2388\n",
      "Epoch 19/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 32.9027 - mean_squared_error: 2083.9778\n",
      "Epoch 20/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 32.6811 - mean_squared_error: 2066.7488\n",
      "Epoch 21/152\n",
      "645/645 [==============================] - 4s 5ms/step - loss: 32.4759 - mean_squared_error: 2049.6963\n",
      "Epoch 22/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 32.2832 - mean_squared_error: 2032.7297\n",
      "Epoch 23/152\n",
      "645/645 [==============================] - 3s 5ms/step - loss: 32.1231 - mean_squared_error: 2019.6204\n",
      "Epoch 24/152\n",
      "645/645 [==============================] - 4s 5ms/step - loss: 31.9776 - mean_squared_error: 2006.6807\n",
      "Epoch 25/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 31.8577 - mean_squared_error: 1997.1259\n",
      "Epoch 26/152\n",
      "645/645 [==============================] - 3s 5ms/step - loss: 31.7488 - mean_squared_error: 1989.3490\n",
      "Epoch 27/152\n",
      "645/645 [==============================] - 4s 7ms/step - loss: 31.6542 - mean_squared_error: 1982.3578\n",
      "Epoch 28/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 31.5669 - mean_squared_error: 1975.7499\n",
      "Epoch 29/152\n",
      "645/645 [==============================] - 3s 5ms/step - loss: 31.4860 - mean_squared_error: 1968.8992\n",
      "Epoch 30/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 31.4154 - mean_squared_error: 1962.7493\n",
      "Epoch 31/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 31.3532 - mean_squared_error: 1957.6755\n",
      "Epoch 32/152\n",
      "645/645 [==============================] - 3s 5ms/step - loss: 31.2960 - mean_squared_error: 1952.3378\n",
      "Epoch 33/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 31.2487 - mean_squared_error: 1948.8578\n",
      "Epoch 34/152\n",
      "645/645 [==============================] - 3s 5ms/step - loss: 31.2047 - mean_squared_error: 1944.4944\n",
      "Epoch 35/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 31.1648 - mean_squared_error: 1941.0095\n",
      "Epoch 36/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 31.1263 - mean_squared_error: 1937.7435\n",
      "Epoch 37/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 31.0916 - mean_squared_error: 1934.8229\n",
      "Epoch 38/152\n",
      "645/645 [==============================] - 3s 5ms/step - loss: 31.0592 - mean_squared_error: 1932.3972\n",
      "Epoch 39/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 31.0287 - mean_squared_error: 1929.0461\n",
      "Epoch 40/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 31.0027 - mean_squared_error: 1926.3943\n",
      "Epoch 41/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 30.9781 - mean_squared_error: 1924.4535\n",
      "Epoch 42/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 30.9542 - mean_squared_error: 1922.3328\n",
      "Epoch 43/152\n",
      "645/645 [==============================] - 4s 7ms/step - loss: 30.9333 - mean_squared_error: 1920.5488\n",
      "Epoch 44/152\n",
      "645/645 [==============================] - 5s 7ms/step - loss: 30.9119 - mean_squared_error: 1918.5348\n",
      "Epoch 45/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 30.8908 - mean_squared_error: 1916.4878\n",
      "Epoch 46/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 30.8706 - mean_squared_error: 1914.9510\n",
      "Epoch 47/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 30.8540 - mean_squared_error: 1914.2433\n",
      "Epoch 48/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 30.8373 - mean_squared_error: 1913.2837\n",
      "Epoch 49/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 30.8197 - mean_squared_error: 1911.8610\n",
      "Epoch 50/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 30.8041 - mean_squared_error: 1910.6583\n",
      "Epoch 51/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 30.7908 - mean_squared_error: 1909.5204\n",
      "Epoch 52/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 30.7742 - mean_squared_error: 1907.7950\n",
      "Epoch 53/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 30.7635 - mean_squared_error: 1906.9342\n",
      "Epoch 54/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 30.7487 - mean_squared_error: 1905.3242\n",
      "Epoch 55/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 30.7351 - mean_squared_error: 1904.3127\n",
      "Epoch 56/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 30.7233 - mean_squared_error: 1903.1655\n",
      "Epoch 57/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 30.7094 - mean_squared_error: 1902.1680\n",
      "Epoch 58/152\n",
      "645/645 [==============================] - 5s 7ms/step - loss: 30.6969 - mean_squared_error: 1900.9271\n",
      "Epoch 59/152\n",
      "645/645 [==============================] - 4s 7ms/step - loss: 30.6848 - mean_squared_error: 1899.8566\n",
      "Epoch 60/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 30.6740 - mean_squared_error: 1898.7617\n",
      "Epoch 61/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 30.6635 - mean_squared_error: 1897.9557\n",
      "Epoch 62/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 30.6539 - mean_squared_error: 1897.3420\n",
      "Epoch 63/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 30.6428 - mean_squared_error: 1896.2133\n",
      "Epoch 64/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 30.6324 - mean_squared_error: 1895.2462\n",
      "Epoch 65/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 30.6224 - mean_squared_error: 1894.4532\n",
      "Epoch 66/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 30.6115 - mean_squared_error: 1893.4545\n",
      "Epoch 67/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 30.6046 - mean_squared_error: 1893.0049\n",
      "Epoch 68/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 30.5951 - mean_squared_error: 1892.2313\n",
      "Epoch 69/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 30.5869 - mean_squared_error: 1891.6223\n",
      "Epoch 70/152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "645/645 [==============================] - 4s 6ms/step - loss: 30.5792 - mean_squared_error: 1890.5885\n",
      "Epoch 71/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 30.5704 - mean_squared_error: 1890.0137\n",
      "Epoch 72/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 30.5613 - mean_squared_error: 1889.2703\n",
      "Epoch 73/152\n",
      "645/645 [==============================] - 4s 7ms/step - loss: 30.5539 - mean_squared_error: 1888.5059\n",
      "Epoch 74/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 30.5457 - mean_squared_error: 1887.9917\n",
      "Epoch 75/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 30.5368 - mean_squared_error: 1887.1969\n",
      "Epoch 76/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 30.5307 - mean_squared_error: 1886.1287\n",
      "Epoch 77/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 30.5204 - mean_squared_error: 1885.6370\n",
      "Epoch 78/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 30.5161 - mean_squared_error: 1885.3126\n",
      "Epoch 79/152\n",
      "645/645 [==============================] - 4s 7ms/step - loss: 30.5063 - mean_squared_error: 1884.4188\n",
      "Epoch 80/152\n",
      "645/645 [==============================] - 4s 7ms/step - loss: 30.5002 - mean_squared_error: 1883.6738\n",
      "Epoch 81/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 30.4940 - mean_squared_error: 1883.2765\n",
      "Epoch 82/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 30.4843 - mean_squared_error: 1882.2903\n",
      "Epoch 83/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 30.4789 - mean_squared_error: 1882.0793\n",
      "Epoch 84/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 30.4726 - mean_squared_error: 1881.2793\n",
      "Epoch 85/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 30.4648 - mean_squared_error: 1880.8690\n",
      "Epoch 86/152\n",
      "645/645 [==============================] - 5s 7ms/step - loss: 30.4588 - mean_squared_error: 1880.3450\n",
      "Epoch 87/152\n",
      "645/645 [==============================] - 6s 9ms/step - loss: 30.4519 - mean_squared_error: 1879.6442\n",
      "Epoch 88/152\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 30.4465 - mean_squared_error: 1879.5244\n",
      "Epoch 89/152\n",
      "645/645 [==============================] - 5s 7ms/step - loss: 30.4408 - mean_squared_error: 1879.1851\n",
      "Epoch 90/152\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 30.4319 - mean_squared_error: 1878.3301\n",
      "Epoch 91/152\n",
      "645/645 [==============================] - 5s 7ms/step - loss: 30.4252 - mean_squared_error: 1877.7233\n",
      "Epoch 92/152\n",
      "645/645 [==============================] - 5s 7ms/step - loss: 30.4190 - mean_squared_error: 1877.3716\n",
      "Epoch 93/152\n",
      "645/645 [==============================] - 5s 7ms/step - loss: 30.4156 - mean_squared_error: 1876.8799\n",
      "Epoch 94/152\n",
      "645/645 [==============================] - 5s 7ms/step - loss: 30.4082 - mean_squared_error: 1876.3398\n",
      "Epoch 95/152\n",
      "645/645 [==============================] - 5s 7ms/step - loss: 30.4035 - mean_squared_error: 1876.0016\n",
      "Epoch 96/152\n",
      "645/645 [==============================] - 5s 7ms/step - loss: 30.3988 - mean_squared_error: 1875.8025\n",
      "Epoch 97/152\n",
      "645/645 [==============================] - 4s 7ms/step - loss: 30.3917 - mean_squared_error: 1875.2101\n",
      "Epoch 98/152\n",
      "645/645 [==============================] - 5s 7ms/step - loss: 30.3864 - mean_squared_error: 1874.8555\n",
      "Epoch 99/152\n",
      "645/645 [==============================] - 5s 7ms/step - loss: 30.3817 - mean_squared_error: 1874.4794\n",
      "Epoch 100/152\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 30.3758 - mean_squared_error: 1874.0299\n",
      "Epoch 101/152\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 30.3705 - mean_squared_error: 1873.4014\n",
      "Epoch 102/152\n",
      "645/645 [==============================] - 5s 7ms/step - loss: 30.3661 - mean_squared_error: 1873.5305\n",
      "Epoch 103/152\n",
      "645/645 [==============================] - 5s 7ms/step - loss: 30.3602 - mean_squared_error: 1873.0483\n",
      "Epoch 104/152\n",
      "645/645 [==============================] - 5s 7ms/step - loss: 30.3548 - mean_squared_error: 1872.3738\n",
      "Epoch 105/152\n",
      "645/645 [==============================] - 5s 7ms/step - loss: 30.3509 - mean_squared_error: 1872.2286\n",
      "Epoch 106/152\n",
      "645/645 [==============================] - 5s 7ms/step - loss: 30.3479 - mean_squared_error: 1872.3236\n",
      "Epoch 107/152\n",
      "645/645 [==============================] - 5s 7ms/step - loss: 30.3419 - mean_squared_error: 1871.9525\n",
      "Epoch 108/152\n",
      "645/645 [==============================] - 5s 7ms/step - loss: 30.3378 - mean_squared_error: 1871.4902\n",
      "Epoch 109/152\n",
      "645/645 [==============================] - 5s 7ms/step - loss: 30.3350 - mean_squared_error: 1871.4150\n",
      "Epoch 110/152\n",
      "645/645 [==============================] - 5s 7ms/step - loss: 30.3283 - mean_squared_error: 1870.8748\n",
      "Epoch 111/152\n",
      "645/645 [==============================] - 5s 7ms/step - loss: 30.3237 - mean_squared_error: 1870.2433\n",
      "Epoch 112/152\n",
      "645/645 [==============================] - 5s 7ms/step - loss: 30.3193 - mean_squared_error: 1869.8977\n",
      "Epoch 113/152\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 30.3139 - mean_squared_error: 1869.6735\n",
      "Epoch 114/152\n",
      "645/645 [==============================] - 6s 9ms/step - loss: 30.3071 - mean_squared_error: 1868.9863\n",
      "Epoch 115/152\n",
      "645/645 [==============================] - 5s 7ms/step - loss: 30.3056 - mean_squared_error: 1868.5968\n",
      "Epoch 116/152\n",
      "645/645 [==============================] - 5s 7ms/step - loss: 30.3019 - mean_squared_error: 1868.4594\n",
      "Epoch 117/152\n",
      "645/645 [==============================] - 5s 7ms/step - loss: 30.2943 - mean_squared_error: 1867.6539\n",
      "Epoch 118/152\n",
      "645/645 [==============================] - 5s 7ms/step - loss: 30.2904 - mean_squared_error: 1867.3949\n",
      "Epoch 119/152\n",
      "645/645 [==============================] - 5s 7ms/step - loss: 30.2847 - mean_squared_error: 1867.2651\n",
      "Epoch 120/152\n",
      "645/645 [==============================] - 5s 7ms/step - loss: 30.2796 - mean_squared_error: 1866.4886\n",
      "Epoch 121/152\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 30.2754 - mean_squared_error: 1866.2753\n",
      "Epoch 122/152\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 30.2712 - mean_squared_error: 1865.9923\n",
      "Epoch 123/152\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 30.2670 - mean_squared_error: 1865.6986\n",
      "Epoch 124/152\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 30.2603 - mean_squared_error: 1864.8882\n",
      "Epoch 125/152\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 30.2559 - mean_squared_error: 1864.5695\n",
      "Epoch 126/152\n",
      "645/645 [==============================] - 6s 9ms/step - loss: 30.2512 - mean_squared_error: 1864.1349\n",
      "Epoch 127/152\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 30.2472 - mean_squared_error: 1863.8383\n",
      "Epoch 128/152\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 30.2427 - mean_squared_error: 1863.5033\n",
      "Epoch 129/152\n",
      "645/645 [==============================] - 5s 7ms/step - loss: 30.2372 - mean_squared_error: 1862.8105\n",
      "Epoch 130/152\n",
      "645/645 [==============================] - 5s 7ms/step - loss: 30.2344 - mean_squared_error: 1862.5732\n",
      "Epoch 131/152\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 30.2309 - mean_squared_error: 1861.8743\n",
      "Epoch 132/152\n",
      "645/645 [==============================] - 4s 7ms/step - loss: 30.2276 - mean_squared_error: 1861.6327\n",
      "Epoch 133/152\n",
      "645/645 [==============================] - 4s 7ms/step - loss: 30.2233 - mean_squared_error: 1861.0671\n",
      "Epoch 134/152\n",
      "645/645 [==============================] - 4s 7ms/step - loss: 30.2175 - mean_squared_error: 1860.8240\n",
      "Epoch 135/152\n",
      "645/645 [==============================] - 4s 7ms/step - loss: 30.2140 - mean_squared_error: 1860.2689\n",
      "Epoch 136/152\n",
      "645/645 [==============================] - 4s 7ms/step - loss: 30.2092 - mean_squared_error: 1860.1080\n",
      "Epoch 137/152\n",
      "645/645 [==============================] - 4s 7ms/step - loss: 30.2042 - mean_squared_error: 1859.3563\n",
      "Epoch 138/152\n",
      "645/645 [==============================] - 4s 7ms/step - loss: 30.1999 - mean_squared_error: 1859.0399\n",
      "Epoch 139/152\n",
      "645/645 [==============================] - 5s 8ms/step - loss: 30.1970 - mean_squared_error: 1858.8765\n",
      "Epoch 140/152\n",
      "645/645 [==============================] - 4s 7ms/step - loss: 30.1931 - mean_squared_error: 1858.4210\n",
      "Epoch 141/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 30.1891 - mean_squared_error: 1857.9943\n",
      "Epoch 142/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 30.1866 - mean_squared_error: 1857.9867\n",
      "Epoch 143/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 30.1814 - mean_squared_error: 1857.6600\n",
      "Epoch 144/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 30.1775 - mean_squared_error: 1857.0146\n",
      "Epoch 145/152\n",
      "645/645 [==============================] - 4s 7ms/step - loss: 30.1741 - mean_squared_error: 1857.0140\n",
      "Epoch 146/152\n",
      "645/645 [==============================] - 4s 7ms/step - loss: 30.1693 - mean_squared_error: 1856.4370\n",
      "Epoch 147/152\n",
      "645/645 [==============================] - 4s 6ms/step - loss: 30.1668 - mean_squared_error: 1856.6398\n",
      "Epoch 148/152\n",
      "645/645 [==============================] - 4s 7ms/step - loss: 30.1638 - mean_squared_error: 1856.1349\n",
      "Epoch 149/152\n",
      "645/645 [==============================] - 4s 7ms/step - loss: 30.1593 - mean_squared_error: 1855.5623\n",
      "Epoch 150/152\n",
      "645/645 [==============================] - 4s 7ms/step - loss: 30.1554 - mean_squared_error: 1855.4968\n",
      "Epoch 151/152\n",
      "645/645 [==============================] - 4s 7ms/step - loss: 30.1513 - mean_squared_error: 1854.7328\n",
      "Epoch 152/152\n",
      "645/645 [==============================] - 4s 7ms/step - loss: 30.1495 - mean_squared_error: 1854.6643\n",
      "INFO:tensorflow:Assets written to: .\\structured_data_regressor\\best_model\\assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ee83001ac0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search = StructuredDataRegressor(max_trials=15, loss='mean_absolute_error') #no of trial and errors allowed\n",
    "search.fit(x=X_train, y=y_train, verbose=1) #fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85efa9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 3ms/step - loss: 22.3890 - mean_squared_error: 893.4369\n"
     ]
    }
   ],
   "source": [
    "mae, acc = search.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e9630fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "yhat = search.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "06099d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4826263253844316\n",
      "22.38899380683899\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.r2_score(y_test, yhat))\n",
    "#print(sklearn.metrics.mean_absolute_percentage_error(y_test, y_pred))\n",
    "print(sklearn.metrics.mean_absolute_error(y_test, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37ea7e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
